{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515d4fb-c376-4109-8581-2ec5454c534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Enable async in Jupyter\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pytz\n",
    "import holidays\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "import logging # Import logging module\n",
    "\n",
    "# Memory threshold (e.g., 90% of available memory)\n",
    "MEMORY_THRESHOLD = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9605f6-8658-41fd-9c2f-a0ab87858115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logging Configuration ---\n",
    "class HKTFormatter(logging.Formatter):\n",
    "    def formatTime(self, record, datefmt=None):\n",
    "        hkt_tz = pytz.timezone('Asia/Hong_Kong')\n",
    "        dt = datetime.fromtimestamp(record.created, tz=pytz.UTC).astimezone(hkt_tz)\n",
    "        return dt.strftime(datefmt or '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logger = logging.getLogger('NewsMonitor')\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler('monitor.log', encoding='utf-8')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = HKTFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "discord_logger = logging.getLogger('DiscordLogger')\n",
    "discord_logger.handlers = []\n",
    "discord_logger.setLevel(logging.INFO)\n",
    "discord_file_handler = logging.FileHandler('discord_messages.log', encoding='utf-8')\n",
    "discord_file_handler.setLevel(logging.INFO)\n",
    "discord_formatter = HKTFormatter('%(asctime)s - News ID: %(news_id)s - Message: %(discord_message)s')\n",
    "discord_file_handler.setFormatter(discord_formatter)\n",
    "discord_logger.addHandler(discord_file_handler)\n",
    "\n",
    "health_check_logger = logging.getLogger('HealthCheckLogger')\n",
    "health_check_logger.handlers = []\n",
    "health_check_logger.setLevel(logging.INFO)\n",
    "health_check_file_handler = logging.FileHandler('health_check.log', encoding='utf-8')\n",
    "health_check_file_handler.setLevel(logging.INFO)\n",
    "health_check_formatter = HKTFormatter('%(asctime)s - %(message)s')\n",
    "health_check_file_handler.setFormatter(health_check_formatter)\n",
    "health_check_logger.addHandler(health_check_file_handler)\n",
    "# --- End Logging Configuration ---\n",
    "\n",
    "# Configuration\n",
    "FINNHUB_API_KEY = \"cumn7d1r01qsapi0gk1gcumn7d1r01qsapi0gk20\"  # Confirmed valid\n",
    "DISCORD_WEBHOOK = \"https://discord.com/api/webhooks/1389596344674680893/h4vaSPCE2I0HyolHgp_EM-b1CIiGwZrm19B7qBLEPd3eHhqYm-J9DfgsT-dHFZkx8PCa\"  # Replace with valid webhook\n",
    "DISCORD_ALERTS = True\n",
    "NEWS_STORAGE_DIR = \"news_data\"\n",
    "METADATA_FILE = os.path.join(NEWS_STORAGE_DIR, \"latest_news_timestamps.json\")\n",
    "TICKERS = [\n",
    "    \"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"TSLA\", \"NVDA\", \"JPM\", \"JNJ\", \"V\", \"PG\",\n",
    "    \"HD\", \"MA\", \"DIS\", \"PYPL\", \"BAC\", \"NFLX\", \"ADBE\", \"CRM\", \"KO\", \"PEP\",\n",
    "    \"TMO\", \"ABT\", \"AVGO\", \"CSCO\", \"CMCSA\", \"XOM\", \"WMT\", \"VZ\", \"MRK\", \"PFE\",\n",
    "    \"INTC\", \"T\", \"ABBV\", \"ORCL\", \"CVX\", \"ACN\", \"DHR\", \"MCD\", \"NKE\", \"PM\"\n",
    "]\n",
    "BATCH_SIZE = 10\n",
    "os.makedirs(NEWS_STORAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bce37-1806-410c-ab3d-413abe53a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsFetcher:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    async def fetch_single_ticker_news(self, ticker, session):\n",
    "        request_time = datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "        current_date = datetime.now(pytz.UTC)\n",
    "        from_date = (current_date - timedelta(days=1)).strftime('%Y-%m-%d')  # Yesterday\n",
    "        to_date = current_date.strftime('%Y-%m-%d')  # Today\n",
    "        url = f\"https://finnhub.io/api/v1/company-news?symbol={ticker}&from={from_date}&to={to_date}&token={self.api_key}\"\n",
    "        logger.debug(f\"Fetching news for {ticker} with URL: {url}\")\n",
    "        \n",
    "        backoff = [2, 4, 8]\n",
    "        for retry in range(3):\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    response_time = datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')\n",
    "                    if response.status == 429:\n",
    "                        delay = backoff[retry] + random.uniform(0.1, 0.5)\n",
    "                        logger.warning(f\"Rate limit hit for {ticker}. Retrying in {delay:.2f}s... (Attempt {retry+1})\")\n",
    "                        await asyncio.sleep(delay)\n",
    "                        continue\n",
    "                    if response.status == 200:\n",
    "                        news = await response.json()\n",
    "                        logger.debug(f\"Raw news for {ticker}: {news}\")\n",
    "                        logger.info(f\"[API] Fetched {len(news)} items for {ticker} - Request: {request_time}, Response: {response_time}\")\n",
    "                        processed_news = []\n",
    "                        for item in news:\n",
    "                            if 'datetime' in item and isinstance(item['datetime'], (int, float)) and 'id' in item:\n",
    "                                item_date = datetime.fromtimestamp(item['datetime'], tz=pytz.UTC)\n",
    "                                # Filter strictly within yesterday to today\n",
    "                                if from_date <= item_date.strftime('%Y-%m-%d') <= to_date:\n",
    "                                    item['ticker'] = ticker\n",
    "                                    item['summary'] = item.get('summary', '')\n",
    "                                    processed_news.append(item)\n",
    "                                else:\n",
    "                                    logger.debug(f\"Skipping item {item.get('id')} for {ticker}: Date {item_date.strftime('%Y-%m-%d')} outside {from_date} to {to_date}\")\n",
    "                            else:\n",
    "                                logger.warning(f\"[Warning] Skipping news item for {ticker} due to missing/invalid 'datetime' or 'id': {item.get('headline', 'N/A')}\")\n",
    "                        if not processed_news:\n",
    "                            logger.info(f\"[Info] No news within range for {ticker} at {response_time}\")\n",
    "                        return processed_news\n",
    "                    else:\n",
    "                        logger.error(f\"[Error] HTTP {response.status} for {ticker} at {response_time}. (Attempt {retry+1})\")\n",
    "                        return []\n",
    "            except aiohttp.ClientError as e:\n",
    "                logger.error(f\"[Error] Network error for {ticker} (retry {retry+1}): {e}\")\n",
    "                await asyncio.sleep(backoff[retry] + random.uniform(0.1, 0.5))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"[Error] Unexpected error for {ticker} (retry {retry+1}): {e}\")\n",
    "                await asyncio.sleep(backoff[retry] + random.uniform(0.1, 0.5))\n",
    "        logger.error(f\"[Error] Failed to fetch news for {ticker} after 3 retries.\")\n",
    "        return []\n",
    "\n",
    "    async def fetch_batch(self, tickers, session):\n",
    "        all_news = []\n",
    "        for ticker in tickers:\n",
    "            await asyncio.sleep(0.1)  # 0.1s delay between calls\n",
    "            result = await self.fetch_single_ticker_news(ticker, session)\n",
    "            if isinstance(result, list):\n",
    "                all_news.extend(result)\n",
    "            else:\n",
    "                logger.error(f\"[Error] Failed to fetch news for {ticker}: {result}\")\n",
    "        return all_news\n",
    "\n",
    "class NewsMonitor:\n",
    "    def __init__(self, tickers, batch_size=10):\n",
    "        self.tickers = tickers\n",
    "        self.batch_size = batch_size\n",
    "        self.fetcher = NewsFetcher(FINNHUB_API_KEY)  # Initialize fetcher here\n",
    "\n",
    "    async def monitor_news(self):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while True:\n",
    "                cycle_start = datetime.now(pytz.UTC)\n",
    "                logger.info(f\"[Chart] Monitoring {len(self.tickers)} stocks (batch size: {self.batch_size})\")\n",
    "                logger.info(\"Starting news monitoring system...\")\n",
    "                \n",
    "                is_market_open = self._is_market_open()\n",
    "                cycle_sleep = 10 if is_market_open else 300  # 10s during market hours, 5min otherwise\n",
    "                \n",
    "                total_items = 0\n",
    "                for i in range(0, len(self.tickers), self.batch_size):\n",
    "                    batch = self.tickers[i:i + self.batch_size]\n",
    "                    logger.debug(f\"Processing batch {batch}\")\n",
    "                    news_items = await self.fetcher.fetch_batch(batch, session)\n",
    "                    total_items += len(news_items)\n",
    "                    logger.info(f\"[Info] Processed {len(news_items)} items for batch {batch}\")\n",
    "                    await asyncio.sleep(1)  # 1-second post-batch sleep\n",
    "\n",
    "                cycle_end = datetime.now(pytz.UTC)\n",
    "                cycle_time = (cycle_end - cycle_start).total_seconds()\n",
    "                logger.info(f\"End of a cycle. Cycle took {cycle_time:.2f} seconds. Total new items: {total_items}\")\n",
    "                \n",
    "                await asyncio.sleep(cycle_sleep)  # Dynamic sleep\n",
    "\n",
    "    def _is_market_open(self):\n",
    "        us_tz = pytz.timezone('US/Eastern')\n",
    "        now = datetime.now(us_tz)\n",
    "        us_holidays = holidays.US(years=now.year)\n",
    "        return now.weekday() <= 4 and 9.5 <= now.hour + now.minute/60 < 16 and now.date() not in us_holidays\n",
    "\n",
    "\n",
    "    def fetch_batch(self, batch):\n",
    "        # Placeholder: Replace with async fetch_batch from NewsFetcher\n",
    "        results = {}\n",
    "        for ticker in batch:\n",
    "            try:\n",
    "                result = {\"items\": [f\"Mock news for {ticker}\"], \"request\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'), \"response\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\n",
    "                results[ticker] = result\n",
    "                logger.info(f\"[API] Fetched {len(result['items'])} items for {ticker}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"[Error] Failed to fetch {ticker}: {e}\")\n",
    "                results[ticker] = {\"items\": [], \"request\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'), \"response\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\n",
    "        time.sleep(1)  # 1-second sleep post-batch\n",
    "        return results\n",
    "\n",
    "    def process_batch(self, results):\n",
    "        total_items = sum(len(v[\"items\"]) for v in results.values())\n",
    "        return total_items\n",
    "\n",
    "class HealthChecker:\n",
    "    def __init__(self, fetcher, processor, tickers):\n",
    "        self.fetcher = fetcher\n",
    "        self.processor = processor\n",
    "        self.tickers = tickers\n",
    "        self.incidents = []\n",
    "\n",
    "    async def log_health_check(self, message):\n",
    "        try:\n",
    "            health_check_logger.info(message)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[Error] Failed to log to HealthCheckLogger: {e}\")\n",
    "            self.incidents.append(f\"Failed to log to health_check.log: {str(e)}\")\n",
    "\n",
    "    async def check_missing_news(self, session, is_market_open):\n",
    "        self.incidents = []\n",
    "        hkt_tz = pytz.timezone('Asia/Hong_Kong')\n",
    "        now_hkt = datetime.now(hkt_tz).strftime('%Y-%m-%d %H:%M HKT')\n",
    "        logger.info(f\"Checking missing news at {now_hkt}...\")\n",
    "        if not self.processor.latest_ticker_timestamps:\n",
    "            logger.warning(f\"News database metadata is empty at {now_hkt}.\")\n",
    "            self.incidents.append(\"News database metadata is empty.\")\n",
    "            await self.log_health_check(f\"News database metadata is empty at {now_hkt}\")\n",
    "            return\n",
    "    \n",
    "        missing_news_alerts = []\n",
    "        for ticker in self.tickers:  # Check all tickers\n",
    "            latest_time = self.processor.latest_ticker_timestamps.get(ticker, 0)\n",
    "            if latest_time == 0:\n",
    "                logger.warning(f\"[Warning] No news timestamp for {ticker}.\")\n",
    "                missing_news_alerts.append(f\"- **{ticker}**: No news timestamp recorded.\")\n",
    "                continue\n",
    "            latest_dt = datetime.fromtimestamp(latest_time, tz=pytz.UTC).astimezone(hkt_tz)\n",
    "            if (datetime.now(hkt_tz) - latest_dt).total_seconds() > 6 * 3600:\n",
    "                missing_news_alerts.append(f\"- **{ticker}**: Latest news is >6h old ({latest_dt.strftime('%Y-%m-%d %H:%M HKT')})\")\n",
    "                logger.warning(f\"[Warning] Missing news for {ticker}: Latest news is >6h old.\")\n",
    "    \n",
    "        if missing_news_alerts:\n",
    "            logger.info(\"Missing news detected for some tickers.\")\n",
    "            self.incidents.append(\"Missing news detected for some tickers.\")\n",
    "            await self.log_health_check(f\"Missing news detected at {now_hkt}: {', '.join(missing_news_alerts)}\")\n",
    "        else:\n",
    "            logger.info(\"No missing news detected for any ticker.\")\n",
    "            await self.log_health_check(f\"No missing news detected at {now_hkt}\")\n",
    "\n",
    "    async def health_check(self, session):\n",
    "        self.incidents = []\n",
    "        hkt_tz = pytz.timezone('Asia/Hong_Kong')\n",
    "        now_hkt = datetime.now(hkt_tz).strftime('%Y-%m-%d %H:%M HKT')\n",
    "        sample_tickers = self.tickers[:3]\n",
    "        raw_news = []\n",
    "        logger.info(f\"Starting health check news fetching for {len(sample_tickers)} tickers at {now_hkt}...\")\n",
    "        try:\n",
    "            news_for_batch = await self.fetcher.fetch_batch(sample_tickers, session)\n",
    "            raw_news.extend(news_for_batch)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[Error] Failed to fetch news for health check: {e}\")\n",
    "            self.incidents.append(f\"Failed to fetch news for health check: {str(e)}\")\n",
    "        logger.info(f\"Finished fetching news for health check. Total items fetched: {len(raw_news)}\")\n",
    "\n",
    "        if not raw_news:\n",
    "            logger.info(f\"[Info] Health check: No news returned for test tickers at {now_hkt}.\")\n",
    "            await self.log_health_check(f\"No news returned for test tickers at {now_hkt}\")\n",
    "            return True\n",
    "\n",
    "        match_count = 0\n",
    "        for item in raw_news:\n",
    "            try:\n",
    "                if self.processor._news_exists(item):\n",
    "                    match_count += 1\n",
    "                else:\n",
    "                    self.incidents.append(f\"News item not found on disk: {item['id']}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"[Error] Error checking news item {item.get('id', 'N/A')} during health check: {e}\")\n",
    "                self.incidents.append(f\"Error checking news item {item.get('id', 'N/A')}: {str(e)}\")\n",
    "\n",
    "        match_percentage = (match_count / len(raw_news)) * 100 if raw_news else 100\n",
    "        logger.info(f\"Health check: {match_percentage:.2f}% match at {now_hkt}\")\n",
    "        await self.log_health_check(f\"Health check: {match_percentage:.2f}% match at {now_hkt}\")\n",
    "        return match_percentage >= 90\n",
    "\n",
    "class Scheduler:\n",
    "    def __init__(self, tickers, batch_size, fetcher, processor, health_checker):\n",
    "        self.tickers = tickers\n",
    "        self.batch_size = batch_size\n",
    "        self.fetcher = fetcher\n",
    "        self.processor = processor\n",
    "        self.health_checker = health_checker\n",
    "        self.last_daily_clear_date = None\n",
    "        self.last_health_check = datetime.min.replace(tzinfo=pytz.UTC)\n",
    "\n",
    "    def _is_market_open(self):\n",
    "        us_tz = pytz.timezone('US/Eastern')\n",
    "        now = datetime.now(us_tz)\n",
    "        us_holidays = holidays.US(years=now.year)\n",
    "        return now.weekday() <= 4 and 9.5 <= now.hour + now.minute/60 < 16 and now.date() not in us_holidays\n",
    "\n",
    "    def _is_friday_6pm(self):\n",
    "        us_tz = pytz.timezone('US/Eastern')\n",
    "        now = datetime.now(us_tz)\n",
    "        return now.weekday() == 4 and now.hour == 18 and now.minute == 0\n",
    "\n",
    "    async def clear_storage(self, now_et):\n",
    "        hkt_tz = pytz.timezone('Asia/Hong_Kong')\n",
    "        now_hkt = datetime.now(hkt_tz).strftime('%Y-%m-%d %H:%M HKT')\n",
    "        logger.info(f\"[Clear] Clearing news data and metadata for {now_hkt}...\")\n",
    "        try:\n",
    "            if os.path.exists(self.processor.storage_dir):\n",
    "                shutil.rmtree(self.processor.storage_dir)\n",
    "                logger.info(f\"Removed directory: {self.processor.storage_dir}\")\n",
    "            os.makedirs(self.processor.storage_dir, exist_ok=True)\n",
    "            self.processor.latest_ticker_timestamps = {}\n",
    "            self.processor._save_metadata()\n",
    "            logger.info(\"Clear completed.\")\n",
    "            self.last_daily_clear_date = now_et.date()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[Error] Failed to clear storage: {e}\")\n",
    "            self.health_checker.incidents.append(f\"Clear storage error: {str(e)}\")\n",
    "\n",
    "    async def run(self):\n",
    "        logger.info(\"Starting news monitoring system...\")\n",
    "        us_tz = pytz.timezone('US/Eastern')\n",
    "        hkt_tz = pytz.timezone('Asia/Hong_Kong')\n",
    "    \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            while True:\n",
    "                try:\n",
    "                    now_utc = datetime.now(pytz.UTC)\n",
    "                    now_et = now_utc.astimezone(us_tz)\n",
    "                    now_hkt = now_utc.astimezone(hkt_tz)\n",
    "                    logger.debug(f\"Current ET: {now_et}, Current HKT: {now_hkt}\")\n",
    "    \n",
    "                    if self._is_friday_6pm():\n",
    "                        await self.clear_storage(now_et)\n",
    "                        await asyncio.sleep(60)\n",
    "                        continue\n",
    "    \n",
    "                    if not self._is_market_open() and now_et.hour >= 16 and (self.last_daily_clear_date is None or self.last_daily_clear_date != now_et.date()):\n",
    "                        await self.clear_storage(now_et)\n",
    "                        await asyncio.sleep(60)\n",
    "                        continue\n",
    "    \n",
    "                    is_market_open = self._is_market_open()\n",
    "                    cycle_sleep = 10 if is_market_open else 300  # 10s during market hours, 5min otherwise\n",
    "                    start_cycle_time = datetime.now(pytz.UTC)\n",
    "                    batch_times = [0, 2, 4, 6]\n",
    "    \n",
    "                    total_new = 0\n",
    "                    for i, batch_start_offset in enumerate(batch_times):\n",
    "                        batch = self.tickers[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                        target_start = start_cycle_time + timedelta(seconds=batch_start_offset)\n",
    "                        while datetime.now(pytz.UTC) < target_start:\n",
    "                            await asyncio.sleep(0.001)\n",
    "                        batch_start_time = datetime.now(pytz.UTC)\n",
    "                        logger.debug(f\"Starting batch {i+1} for tickers {batch} at {batch_start_time}\")\n",
    "                        try:\n",
    "                            news_items = await self.fetcher.fetch_batch(batch, session)\n",
    "                            logger.debug(f\"Fetched {len(news_items)} items for batch {batch}\")\n",
    "                            total_new += await self.processor.process_batch(news_items, session)\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"[Error] Failed to process batch {batch}: {e}\")\n",
    "                            self.health_checker.incidents.append(f\"Batch processing error: {str(e)}\")\n",
    "                        \n",
    "                        batch_end_time = datetime.now(pytz.UTC)\n",
    "                        logger.info(f\"End of batch {i+1}\")\n",
    "                        batch_duration = (batch_end_time - batch_start_time).total_seconds()\n",
    "                        sleep_for_pacing = max(0.0, 1.0 - batch_duration)\n",
    "                        if sleep_for_pacing > 0:\n",
    "                            await asyncio.sleep(sleep_for_pacing)\n",
    "    \n",
    "                    cycle_end_time = datetime.now(pytz.UTC)\n",
    "                    cycle_duration = (cycle_end_time - start_cycle_time).total_seconds()\n",
    "                    logger.info(f\"End of a cycle. Cycle took {cycle_duration:.2f} seconds. Total new items: {total_new}\")\n",
    "    \n",
    "                    if self._is_health_check_time(now_hkt, is_market_open):\n",
    "                        now_hkt_str = now_hkt.strftime('%Y-%m-%d %H:%M HKT')\n",
    "                        logger.info(f\"Running missing news check at {now_hkt_str}...\")\n",
    "                        try:\n",
    "                            await self.health_checker.check_missing_news(session, is_market_open)\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"[Error] Failed to check missing news: {e}\")\n",
    "                        logger.info(f\"Running health check at {now_hkt_str}...\")\n",
    "                        try:\n",
    "                            await self.health_checker.health_check(session)\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"[Error] Health check error at {now_hkt_str}: {e}\")\n",
    "                        self.last_health_check = now_utc\n",
    "    \n",
    "                    if cycle_duration < cycle_sleep:\n",
    "                        logger.info(f\"Sleeping for {cycle_sleep - cycle_duration:.2f} seconds after cycle\")\n",
    "                        await asyncio.sleep(cycle_sleep - cycle_duration)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"[Critical] Scheduler loop crashed: {e}\")\n",
    "                    await asyncio.sleep(60)\n",
    "\n",
    "    def _is_health_check_time(self, now_hkt, is_market_open):\n",
    "        minute = now_hkt.minute\n",
    "        hour = now_hkt.hour\n",
    "        if is_market_open:  # 21:30 HKT–04:00 HKT\n",
    "            return minute in (0, 30)  # Every 30 minutes (e.g., 22:00, 22:30)\n",
    "        else:  # U.S. off-market hours\n",
    "            return minute == 0  # Every 60 minutes (e.g., 07:00, 08:00)\n",
    "\n",
    "class NewsProcessor:\n",
    "    def __init__(self, storage_dir, discord_webhook, discord_alerts):\n",
    "        self.storage_dir = storage_dir\n",
    "        self.discord_webhook = discord_webhook\n",
    "        self.discord_alerts = discord_alerts\n",
    "        self.discord_enabled = bool(discord_webhook)\n",
    "        os.makedirs(storage_dir, exist_ok=True)\n",
    "        self.latest_ticker_timestamps = self._load_metadata()\n",
    "        logger.debug(f\"NewsProcessor initialized: storage_dir={storage_dir}\")\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        metadata_path = os.path.join(self.storage_dir, \"latest_news_timestamps.json\")\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "\n",
    "    def _save_metadata(self):\n",
    "        metadata_path = os.path.join(self.storage_dir, \"latest_news_timestamps.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.latest_ticker_timestamps, f)\n",
    "\n",
    "    def _generate_id(self, item):\n",
    "        if 'datetime' not in item or not isinstance(item['datetime'], (int, float)):\n",
    "            ts_str = datetime.now().strftime('%Y%m%d%H%M')\n",
    "        else:\n",
    "            ts_str = datetime.fromtimestamp(item['datetime'], tz=pytz.UTC).strftime('%Y%m%d%H%M')\n",
    "        ticker_val = item.get('ticker', 'UNKNOWN_TICKER')\n",
    "        headline_val = item.get('headline', 'UNKNOWN_HEADLINE').encode()\n",
    "        return f\"{ticker_val}_{ts_str}_{hashlib.md5(headline_val).hexdigest()[:6]}\"\n",
    "\n",
    "    def _get_news_filepath(self, item):\n",
    "        finnhub_id = item.get('id')\n",
    "        if not finnhub_id:\n",
    "            logger.warning(f\"Finnhub ID missing for item {item.get('headline', 'N/A')}, using generated ID as fallback\")\n",
    "            finnhub_id = self._generate_id(item)\n",
    "        timestamp = item.get('datetime', 0)\n",
    "        if not isinstance(timestamp, (int, float)):\n",
    "            date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            date_str = datetime.fromtimestamp(timestamp, tz=pytz.UTC).strftime('%Y-%m-%d')\n",
    "        ticker = item.get('ticker', 'UNKNOWN_TICKER')\n",
    "        date_dir = os.path.join(self.storage_dir, date_str)\n",
    "        ticker_dir = os.path.join(date_dir, ticker)\n",
    "        os.makedirs(ticker_dir, exist_ok=True)\n",
    "        return os.path.join(ticker_dir, f\"{finnhub_id}.json\")\n",
    "\n",
    "    async def process_batch(self, news_items, session):\n",
    "        logger.info(f\"Processing batch with {len(news_items)} items\")\n",
    "        total_new = 0\n",
    "        hkt_tz = pytz.timezone('Asia/Hong_Kong')\n",
    "        for item in news_items:\n",
    "            ticker = item.get('ticker')\n",
    "            if not ticker:\n",
    "                logger.warning(f\"Skipping news item with no ticker: {item.get('headline', 'N/A')}\")\n",
    "                continue\n",
    "            timestamp = item.get('datetime', 0)\n",
    "            if not isinstance(timestamp, (int, float)):\n",
    "                logger.warning(f\"Skipping news item for {ticker} with invalid timestamp: {item.get('headline', 'N/A')}\")\n",
    "                continue\n",
    "\n",
    "            # Generate or validate ID\n",
    "            item_id = item.get('id')\n",
    "            if not item_id:\n",
    "                item_id = self._generate_id(item)\n",
    "                item['id'] = item_id\n",
    "                logger.info(f\"Generated ID {item_id} for news item {item.get('headline', 'N/A')}\")\n",
    "\n",
    "            # Standardize JSON structure\n",
    "            standardized_item = {\n",
    "                'id': item_id,\n",
    "                'ticker': ticker,\n",
    "                'headline': item.get('headline', 'No headline'),\n",
    "                'datetime': timestamp,\n",
    "                'url': item.get('url', ''),\n",
    "                'summary': item.get('summary', ''),\n",
    "                'source': item.get('source', 'Unknown')\n",
    "            }\n",
    "\n",
    "            # Check if new item\n",
    "            if ticker not in self.latest_ticker_timestamps or timestamp > self.latest_ticker_timestamps.get(ticker, 0):\n",
    "                self.latest_ticker_timestamps[ticker] = timestamp\n",
    "                total_new += 1\n",
    "                logger.info(f\"New news for {ticker}: {standardized_item['headline']}\")\n",
    "\n",
    "                # Save to disk with standardized structure\n",
    "                storage_path = self._get_news_filepath(standardized_item)\n",
    "                try:\n",
    "                    with open(storage_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(standardized_item, f, ensure_ascii=False, indent=2)\n",
    "                    logger.debug(f\"Saved news item {finnhub_id} for {ticker} to {storage_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to save news item {finnhub_id} for {ticker}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Send Discord alert\n",
    "                if self.discord_enabled and self.discord_alerts:\n",
    "                    hkt_time = datetime.fromtimestamp(timestamp, tz=pytz.UTC).astimezone(hkt_tz).strftime('%Y-%m-%d %H:%M HKT')\n",
    "                    message = (\n",
    "                        f\"**{ticker} News**:\\n\"\n",
    "                        f\"**Title**: {standardized_item['headline']}\\n\"\n",
    "                        f\"**Time**: {hkt_time}\\n\"\n",
    "                        f\"**URL**: {standardized_item['url']}\\n\"\n",
    "                        f\"**Summary**: {standardized_item['summary']}\\n\"\n",
    "                        f\"**Source**: {standardized_item['source']}\"\n",
    "                    )\n",
    "                    payload = {\"content\": message}\n",
    "                    try:\n",
    "                        async with session.post(self.discord_webhook, json=payload) as response:\n",
    "                            if response.status == 204:\n",
    "                                discord_logger.info(f\"Posted to Discord: {item_id}\", extra={\"news_id\": item_id, \"discord_message\": message})\n",
    "                            else:\n",
    "                                logger.error(f\"Failed to post to Discord: HTTP {response.status}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error posting to Discord: {e}\")\n",
    "\n",
    "        self._save_metadata()\n",
    "        logger.debug(f\"Batch processed, total new items: {total_new}\")\n",
    "        return total_new\n",
    "\n",
    "    async def _send_discord_alert(self, item, session):\n",
    "        # This method is now handled within process_batch for consistency\n",
    "        pass\n",
    "\n",
    "    def _news_exists(self, item):\n",
    "        filepath = self._get_news_filepath(item)\n",
    "        return os.path.exists(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b0b40-5811-49bd-97cb-dd0a7efeeb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    fetcher = NewsFetcher(FINNHUB_API_KEY)\n",
    "    processor = NewsProcessor(NEWS_STORAGE_DIR, DISCORD_WEBHOOK, DISCORD_ALERTS)\n",
    "    health_checker = HealthChecker(fetcher, processor, TICKERS)\n",
    "    scheduler = Scheduler(TICKERS, BATCH_SIZE, fetcher, processor, health_checker)\n",
    "    await scheduler.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3dacfa-69fc-4736-aa72-3c960c7f3524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc883b-7f94-479a-abb5-bec4f98e9d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
